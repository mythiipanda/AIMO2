{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07e8879e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-25T02:51:38.263408Z",
     "iopub.status.busy": "2024-10-25T02:51:38.262446Z",
     "iopub.status.idle": "2024-10-25T02:51:38.269929Z",
     "shell.execute_reply": "2024-10-25T02:51:38.268590Z",
     "shell.execute_reply.started": "2024-10-25T02:51:38.263362Z"
    },
    "papermill": {
     "duration": 0.006405,
     "end_time": "2024-10-29T09:38:32.008077",
     "exception": false,
     "start_time": "2024-10-29T09:38:32.001672",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "References\n",
    "\n",
    "- https://www.kaggle.com/code/richolson/ai-math-olympiad-qwen2-5-72b for showing how to submit\n",
    "- https://www.kaggle.com/code/abdullahmeda/load-72b-awq-model-using-vllm-on-l4-x4\n",
    "- https://www.kaggle.com/code/huikang/qwen2-5-math-1-5b-instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43999c03",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-29T09:38:32.020598Z",
     "iopub.status.busy": "2024-10-29T09:38:32.020311Z",
     "iopub.status.idle": "2024-10-29T09:38:48.272150Z",
     "shell.execute_reply": "2024-10-29T09:38:48.271374Z"
    },
    "papermill": {
     "duration": 16.260394,
     "end_time": "2024-10-29T09:38:48.274309",
     "exception": false,
     "start_time": "2024-10-29T09:38:32.013915",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "\n",
    "import torch\n",
    "import kaggle_evaluation.aimo_2_inference_server\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74e54e7f",
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2024-10-29T09:38:48.287715Z",
     "iopub.status.busy": "2024-10-29T09:38:48.287044Z",
     "iopub.status.idle": "2024-10-29T09:44:36.774109Z",
     "shell.execute_reply": "2024-10-29T09:44:36.773207Z"
    },
    "papermill": {
     "duration": 348.495875,
     "end_time": "2024-10-29T09:44:36.776290",
     "exception": false,
     "start_time": "2024-10-29T09:38:48.280415",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-29 09:39:26,419\tINFO util.py:124 -- Outdated packages:\n",
      "  ipywidgets==7.7.1 found, needs ipywidgets>=8\n",
      "Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 10-29 09:39:31 awq_marlin.py:89] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.\n",
      "INFO 10-29 09:39:31 config.py:904] Defaulting to use mp for distributed inference\n",
      "INFO 10-29 09:39:31 llm_engine.py:223] Initializing an LLM engine (v0.6.1.post2) with config: model='/kaggle/input/qwen2.5/transformers/72b-instruct-awq/1', speculative_config=None, tokenizer='/kaggle/input/qwen2.5/transformers/72b-instruct-awq/1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=awq_marlin, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/kaggle/input/qwen2.5/transformers/72b-instruct-awq/1, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)\n",
      "WARNING 10-29 09:39:31 multiproc_gpu_executor.py:56] Reducing Torch parallelism from 24 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 10-29 09:39:31 custom_cache_manager.py:17] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager\n",
      "\u001b[1;36m(VllmWorkerProcess pid=245)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=244)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=243)\u001b[0;0m INFO 10-29 09:39:33 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n",
      "INFO 10-29 09:39:33 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n",
      "INFO 10-29 09:39:33 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n",
      "INFO 10-29 09:39:34 utils.py:981] Found nccl from library libnccl.so.2\n",
      "INFO 10-29 09:39:34 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=243)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=244)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=245)\u001b[0;0m INFO 10-29 09:39:34 utils.py:981] Found nccl from library libnccl.so.2\n",
      "INFO 10-29 09:39:34 utils.py:981] Found nccl from library libnccl.so.2\n",
      "INFO 10-29 09:39:34 utils.py:981] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorkerProcess pid=243)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=245)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=244)\u001b[0;0m INFO 10-29 09:39:34 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "INFO 10-29 09:39:34 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "INFO 10-29 09:39:34 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "WARNING 10-29 09:39:35 custom_all_reduce.py:122] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=243)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=244)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=245)\u001b[0;0m WARNING 10-29 09:39:35 custom_all_reduce.py:122] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 10-29 09:39:35 custom_all_reduce.py:122] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 10-29 09:39:35 custom_all_reduce.py:122] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "INFO 10-29 09:39:35 shm_broadcast.py:235] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1, 2, 3], buffer=<vllm.distributed.device_communicators.shm_broadcast.ShmRingBuffer object at 0x7b04e89145b0>, local_subscribe_port=42799, remote_subscribe_port=None)\n",
      "INFO 10-29 09:39:35 model_runner.py:997] Starting to load model /kaggle/input/qwen2.5/transformers/72b-instruct-awq/1...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=245)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=243)\u001b[0;0m INFO 10-29 09:39:35 model_runner.py:997] Starting to load model /kaggle/input/qwen2.5/transformers/72b-instruct-awq/1...\n",
      "INFO 10-29 09:39:35 model_runner.py:997] Starting to load model /kaggle/input/qwen2.5/transformers/72b-instruct-awq/1...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=244)\u001b[0;0m INFO 10-29 09:39:35 model_runner.py:997] Starting to load model /kaggle/input/qwen2.5/transformers/72b-instruct-awq/1...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36f93f6a1b2d441dbdb38bf01b89ea32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/11 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorkerProcess pid=245)\u001b[0;0m INFO 10-29 09:44:20 model_runner.py:1008] Loading model weights took 9.7897 GB\n",
      "INFO 10-29 09:44:20 model_runner.py:1008] Loading model weights took 9.7897 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=243)\u001b[0;0m INFO 10-29 09:44:20 model_runner.py:1008] Loading model weights took 9.7897 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=244)\u001b[0;0m INFO 10-29 09:44:20 model_runner.py:1008] Loading model weights took 9.7897 GB\n",
      "INFO 10-29 09:44:27 distributed_gpu_executor.py:57] # GPU blocks: 9006, # CPU blocks: 3276\n",
      "INFO 10-29 09:44:30 model_runner.py:1311] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 10-29 09:44:30 model_runner.py:1315] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=243)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=245)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=244)\u001b[0;0m INFO 10-29 09:44:30 model_runner.py:1311] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 10-29 09:44:30 model_runner.py:1311] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 10-29 09:44:30 model_runner.py:1311] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=243)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=245)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=244)\u001b[0;0m INFO 10-29 09:44:30 model_runner.py:1315] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 10-29 09:44:30 model_runner.py:1315] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 10-29 09:44:30 model_runner.py:1315] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 10-29 09:44:36 model_runner.py:1430] Graph capturing finished in 6 secs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=243)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=244)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=245)\u001b[0;0m INFO 10-29 09:44:36 model_runner.py:1430] Graph capturing finished in 6 secs.\n",
      "INFO 10-29 09:44:36 model_runner.py:1430] Graph capturing finished in 6 secs.\n",
      "INFO 10-29 09:44:36 model_runner.py:1430] Graph capturing finished in 6 secs.\n"
     ]
    }
   ],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1,2,3\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "def clean_memory(deep=False):\n",
    "    gc.collect()\n",
    "    if deep:\n",
    "        ctypes.CDLL(\"libc.so.6\").malloc_trim(0)\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "llm_model_pth = '/kaggle/input/qwen2.5/transformers/72b-instruct-awq/1'\n",
    "\n",
    "llm = LLM(\n",
    "    llm_model_pth,\n",
    "    dtype=\"half\",                # The data type for the model weights and activations\n",
    "    max_num_seqs=16,             # Maximum number of sequences per iteration. Default is 256\n",
    "    max_model_len=4096,          # Model context length\n",
    "    trust_remote_code=True,      # Trust remote code (e.g., from HuggingFace) when downloading the model and tokenizer\n",
    "    tensor_parallel_size=4,      # The number of GPUs to use for distributed execution with tensor parallelism\n",
    "    gpu_memory_utilization=0.97, # The ratio (between 0 and 1) of GPU memory to reserve for the model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf7529cf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-29T09:44:36.797961Z",
     "iopub.status.busy": "2024-10-29T09:44:36.797623Z",
     "iopub.status.idle": "2024-10-29T09:44:36.801567Z",
     "shell.execute_reply": "2024-10-29T09:44:36.800907Z"
    },
    "papermill": {
     "duration": 0.016614,
     "end_time": "2024-10-29T09:44:36.803215",
     "exception": false,
     "start_time": "2024-10-29T09:44:36.786601",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = llm.get_tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b5a2130",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-29T09:44:36.824335Z",
     "iopub.status.busy": "2024-10-29T09:44:36.823714Z",
     "iopub.status.idle": "2024-10-29T09:44:36.831953Z",
     "shell.execute_reply": "2024-10-29T09:44:36.831262Z"
    },
    "papermill": {
     "duration": 0.020467,
     "end_time": "2024-10-29T09:44:36.833553",
     "exception": false,
     "start_time": "2024-10-29T09:44:36.813086",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "def extract_python_code(text):\n",
    "    pattern = r'```python\\s*(.*?)\\s*```'\n",
    "    matches = re.findall(pattern, text, re.DOTALL)\n",
    "    return \"\\n\\n\".join(matches)\n",
    "\n",
    "\n",
    "def process_python_code(query):\n",
    "    query = \"import math\\nimport numpy as np\\nimport sympy as sp\\n\" + query\n",
    "    current_rows = query.strip().split(\"\\n\")\n",
    "    new_rows = []\n",
    "    for row in current_rows:\n",
    "        new_rows.append(row)\n",
    "        if not row.startswith(\" \") and \"=\" in row:\n",
    "                variable_to_print = row.split(\"=\")[0].strip()\n",
    "                new_rows.append(f'print(f\"{{{variable_to_print}=}}\")')\n",
    "    return \"\\n\".join(new_rows)\n",
    "\n",
    "\n",
    "def extract_boxed_text(text):\n",
    "    pattern = r'oxed{(.*?)}'\n",
    "    matches = re.findall(pattern, text)\n",
    "    if not matches:\n",
    "        return \"\"\n",
    "    return matches[0]\n",
    "\n",
    "\n",
    "from collections import Counter\n",
    "import random\n",
    "def select_answer(answers):\n",
    "    counter = Counter()\n",
    "    for answer in answers:\n",
    "        try:\n",
    "            if int(answer) == float(answer):\n",
    "                counter[int(answer)] += 1 + random.random() / 1_000\n",
    "        except:\n",
    "            pass\n",
    "    if not counter:\n",
    "        return 210\n",
    "    _, answer = sorted([(v,k) for k,v in counter.items()], reverse=True)[0]\n",
    "    return answer%1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "56731764",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-29T09:44:36.854116Z",
     "iopub.status.busy": "2024-10-29T09:44:36.853856Z",
     "iopub.status.idle": "2024-10-29T09:44:36.861088Z",
     "shell.execute_reply": "2024-10-29T09:44:36.860434Z"
    },
    "papermill": {
     "duration": 0.019294,
     "end_time": "2024-10-29T09:44:36.862680",
     "exception": false,
     "start_time": "2024-10-29T09:44:36.843386",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import tempfile\n",
    "import subprocess\n",
    "\n",
    "class PythonREPL:\n",
    "    def __init__(self, timeout=5):\n",
    "        self.timeout = timeout\n",
    "\n",
    "    def __call__(self, query):\n",
    "        with tempfile.TemporaryDirectory() as temp_dir:\n",
    "            temp_file_path = os.path.join(temp_dir, \"tmp.py\")\n",
    "            with open(temp_file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(query)\n",
    "            \n",
    "            try:\n",
    "                result = subprocess.run(\n",
    "                    [\"python3\", temp_file_path],\n",
    "                    capture_output=True,\n",
    "                    check=False,\n",
    "                    text=True,\n",
    "                    timeout=self.timeout,\n",
    "                )\n",
    "            except subprocess.TimeoutExpired:\n",
    "                return False, f\"Execution timed out after {self.timeout} seconds.\"\n",
    "\n",
    "            if result.returncode == 0:\n",
    "                output = result.stdout.strip()\n",
    "                return True, output\n",
    "            else:\n",
    "                error_msg = result.stderr.strip()\n",
    "                # Process the error message to remove the temporary file path\n",
    "                # This makes the error message cleaner and more user-friendly\n",
    "                error_lines = error_msg.split(\"\\n\")\n",
    "                cleaned_errors = []\n",
    "                for line in error_lines:\n",
    "                    if temp_file_path in line:\n",
    "                        # Remove the path from the error line\n",
    "                        line = line.replace(temp_file_path, \"<temporary_file>\")\n",
    "                    cleaned_errors.append(line)\n",
    "                cleaned_error_msg = \"\\n\".join(cleaned_errors)\n",
    "                return False, cleaned_error_msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "41ede126",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-29T09:44:36.883866Z",
     "iopub.status.busy": "2024-10-29T09:44:36.883230Z",
     "iopub.status.idle": "2024-10-29T09:44:36.888978Z",
     "shell.execute_reply": "2024-10-29T09:44:36.888302Z"
    },
    "papermill": {
     "duration": 0.017836,
     "end_time": "2024-10-29T09:44:36.890530",
     "exception": false,
     "start_time": "2024-10-29T09:44:36.872694",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sampling_params = SamplingParams(\n",
    "    temperature=1.0,              # randomness of the sampling\n",
    "    min_p=0.01,\n",
    "    skip_special_tokens=True,     # Whether to skip special tokens in the output.\n",
    "    max_tokens=1800,\n",
    "    stop=[\"```\\n\"],\n",
    "    include_stop_str_in_output=True,\n",
    ")\n",
    "\n",
    "def batch_message_generate(list_of_messages) -> list[list[dict]]:\n",
    "\n",
    "    list_of_texts = [\n",
    "        tokenizer.apply_chat_template(\n",
    "            conversation=messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True\n",
    "        )\n",
    "        for messages in list_of_messages\n",
    "    ]\n",
    "    \n",
    "    request_output = llm.generate(\n",
    "        prompts=list_of_texts,\n",
    "        sampling_params=sampling_params,\n",
    "    )\n",
    "    \n",
    "    for messages, single_request_output in zip(list_of_messages, request_output):\n",
    "        # print()\n",
    "        # print(single_request_output.outputs[0].text)\n",
    "        # print()\n",
    "        messages.append({'role': 'assistant', 'content': single_request_output.outputs[0].text})\n",
    "\n",
    "    return list_of_messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1234907a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-29T09:44:36.911266Z",
     "iopub.status.busy": "2024-10-29T09:44:36.910715Z",
     "iopub.status.idle": "2024-10-29T09:44:36.915120Z",
     "shell.execute_reply": "2024-10-29T09:44:36.914475Z"
    },
    "papermill": {
     "duration": 0.016399,
     "end_time": "2024-10-29T09:44:36.916670",
     "exception": false,
     "start_time": "2024-10-29T09:44:36.900271",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def batch_message_filter(list_of_messages) -> tuple[list[list[dict]], list[str]]:\n",
    "    extracted_answers = []\n",
    "    list_of_messages_to_keep = []\n",
    "    for messages in list_of_messages:\n",
    "        answer = extract_boxed_text(messages[-1]['content'])\n",
    "        if answer:\n",
    "            extracted_answers.append(answer)\n",
    "        else:\n",
    "            list_of_messages_to_keep.append(messages)\n",
    "    return list_of_messages_to_keep, extracted_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f2121b0c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-29T09:44:36.937783Z",
     "iopub.status.busy": "2024-10-29T09:44:36.937116Z",
     "iopub.status.idle": "2024-10-29T09:44:36.942693Z",
     "shell.execute_reply": "2024-10-29T09:44:36.942055Z"
    },
    "papermill": {
     "duration": 0.017826,
     "end_time": "2024-10-29T09:44:36.944287",
     "exception": false,
     "start_time": "2024-10-29T09:44:36.926461",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def batch_message_execute(list_of_messages) -> list[list[dict]]:\n",
    "    for messages in list_of_messages:\n",
    "        python_code = extract_python_code(messages[-1]['content'])\n",
    "        python_code = process_python_code(python_code)\n",
    "        # print('\\n\\n' + python_code + '\\n\\n')\n",
    "        try:\n",
    "            print('c', end='')\n",
    "            is_successful, output = PythonREPL()(python_code)\n",
    "            if is_successful:\n",
    "                print('o', end='')\n",
    "            else:\n",
    "                print('e', end='')\n",
    "        except Exception as e:\n",
    "            print('f', end='')\n",
    "            output = str(e)\n",
    "        # print(output)\n",
    "        messages.append({'role': 'user', 'content': output})\n",
    "    print()\n",
    "    return list_of_messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "91081e29",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-29T09:44:36.965018Z",
     "iopub.status.busy": "2024-10-29T09:44:36.964504Z",
     "iopub.status.idle": "2024-10-29T09:44:36.967995Z",
     "shell.execute_reply": "2024-10-29T09:44:36.967348Z"
    },
    "papermill": {
     "duration": 0.015495,
     "end_time": "2024-10-29T09:44:36.969571",
     "exception": false,
     "start_time": "2024-10-29T09:44:36.954076",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "\n",
    "import kaggle_evaluation.aimo_2_inference_server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5e914387",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-29T09:44:36.990854Z",
     "iopub.status.busy": "2024-10-29T09:44:36.990278Z",
     "iopub.status.idle": "2024-10-29T09:44:36.995851Z",
     "shell.execute_reply": "2024-10-29T09:44:36.995207Z"
    },
    "papermill": {
     "duration": 0.017867,
     "end_time": "2024-10-29T09:44:36.997383",
     "exception": false,
     "start_time": "2024-10-29T09:44:36.979516",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_starter_messages(question, index):\n",
    "    cycle_size = 4\n",
    "    if False:\n",
    "        pass\n",
    "    elif index % cycle_size == 3:\n",
    "        return [\n",
    "            {\"role\": \"user\", \"content\": \"Translate this problem into Python code.\\n\\n\" + question + \"\\n\\nStart by importing numpy and sympy. If you get a confident answer after running the sympy code, put your final answer within \\boxed{}\"}\n",
    "        ]\n",
    "    elif index % cycle_size == 2:\n",
    "        return [\n",
    "            {\"role\": \"user\", \"content\": \"Translate the following problem into sympy.\\n\\n\" + question + \"\\n\\nStart by importing sympy. If you get a confident answer after running the sympy code, put your final answer within \\boxed{}\"}\n",
    "        ]\n",
    "    elif index % cycle_size == 1:\n",
    "        # https://github.com/QwenLM/Qwen2.5-Math?tab=readme-ov-file#-hugging-face-transformers\n",
    "        return [\n",
    "            {\"role\": \"system\", \"content\": \"Please reason step by step, and put your final answer within \\\\boxed{}.\"},\n",
    "            {\"role\": \"user\", \"content\": question}\n",
    "        ]\n",
    "    else:\n",
    "        # https://github.com/QwenLM/Qwen2.5-Math?tab=readme-ov-file#-hugging-face-transformers\n",
    "        return [\n",
    "            {\"role\": \"system\", \"content\": \"Please integrate natural language reasoning with Python programs to solve the problem above, and put your final answer within \\\\boxed{}.\"},\n",
    "            {\"role\": \"user\", \"content\": question}\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3e5a87b9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-29T09:44:37.018015Z",
     "iopub.status.busy": "2024-10-29T09:44:37.017479Z",
     "iopub.status.idle": "2024-10-29T09:44:37.023237Z",
     "shell.execute_reply": "2024-10-29T09:44:37.022591Z"
    },
    "papermill": {
     "duration": 0.017773,
     "end_time": "2024-10-29T09:44:37.024843",
     "exception": false,
     "start_time": "2024-10-29T09:44:37.007070",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predict_for_question(question: str) -> int:\n",
    "    import os\n",
    "    if not os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n",
    "        if question != \"Triangle $ABC$ has side length $AB = 120$ and circumradius $R = 100$. Let $D$ be the foot of the perpendicular from $C$ to the line $AB$. What is the greatest possible length of segment $CD$?\":\n",
    "            return 210\n",
    "    \n",
    "    question += \"\\nIf the final answer is a number larger than 1 million, take modulo 1000.\"\n",
    "    print(question)\n",
    "\n",
    "    list_of_messages = [create_starter_messages(question, index) for index in range(16)]\n",
    "\n",
    "    all_extracted_answers = []\n",
    "    for _ in range(4):\n",
    "        list_of_messages = batch_message_generate(list_of_messages)\n",
    "        list_of_messages, extracted_answers = batch_message_filter(list_of_messages)\n",
    "        all_extracted_answers.extend(extracted_answers)\n",
    "        if not list_of_messages:\n",
    "            break\n",
    "        list_of_messages = batch_message_execute(list_of_messages)\n",
    "\n",
    "    print(all_extracted_answers)\n",
    "    answer = select_answer(all_extracted_answers)\n",
    "    print(answer)\n",
    "\n",
    "    print(\"\\n\\n\")\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "187f0aeb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-29T09:44:37.045552Z",
     "iopub.status.busy": "2024-10-29T09:44:37.044986Z",
     "iopub.status.idle": "2024-10-29T09:44:37.049563Z",
     "shell.execute_reply": "2024-10-29T09:44:37.048928Z"
    },
    "papermill": {
     "duration": 0.016526,
     "end_time": "2024-10-29T09:44:37.051151",
     "exception": false,
     "start_time": "2024-10-29T09:44:37.034625",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Replace this function with your inference code.\n",
    "# The function should return a single integer between 0 and 999, inclusive.\n",
    "# Each prediction (except the very first) must be returned within 30 minutes of the question being provided.\n",
    "def predict(id_: pl.DataFrame, question: pl.DataFrame) -> pl.DataFrame | pd.DataFrame:\n",
    "    id_ = id_.item(0)\n",
    "    print(\"------\")\n",
    "    print(id_)\n",
    "    \n",
    "    question = question.item(0)\n",
    "    answer = predict_for_question(question)\n",
    "    print(question)\n",
    "    print(\"------\\n\\n\\n\")\n",
    "    return pl.DataFrame({'id': id_, 'answer': answer})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "75e2193e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-29T09:44:37.071917Z",
     "iopub.status.busy": "2024-10-29T09:44:37.071325Z",
     "iopub.status.idle": "2024-10-29T09:44:37.074651Z",
     "shell.execute_reply": "2024-10-29T09:44:37.074015Z"
    },
    "papermill": {
     "duration": 0.015514,
     "end_time": "2024-10-29T09:44:37.076325",
     "exception": false,
     "start_time": "2024-10-29T09:44:37.060811",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# predict_for_question(\"Triangle $ABC$ has side length $AB = 120$ and circumradius $R = 100$. Let $D$ be the foot of the perpendicular from $C$ to the line $AB$. What is the greatest possible length of segment $CD$?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a591eb11",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-29T09:44:37.097161Z",
     "iopub.status.busy": "2024-10-29T09:44:37.096599Z",
     "iopub.status.idle": "2024-10-29T09:44:37.158304Z",
     "shell.execute_reply": "2024-10-29T09:44:37.157612Z"
    },
    "papermill": {
     "duration": 0.073902,
     "end_time": "2024-10-29T09:44:37.160039",
     "exception": false,
     "start_time": "2024-10-29T09:44:37.086137",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "pd.read_csv(\n",
    "    '/kaggle/input/ai-mathematical-olympiad-progress-prize-2/reference.csv'\n",
    ").drop('answer', axis=1).to_csv('reference.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "19fed795",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-29T09:44:37.180676Z",
     "iopub.status.busy": "2024-10-29T09:44:37.180410Z",
     "iopub.status.idle": "2024-10-29T09:47:53.870631Z",
     "shell.execute_reply": "2024-10-29T09:47:53.869831Z"
    },
    "papermill": {
     "duration": 196.702594,
     "end_time": "2024-10-29T09:47:53.872530",
     "exception": false,
     "start_time": "2024-10-29T09:44:37.169936",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------\n",
      "a1d40b\n",
      "The Fibonacci numbers are defined as follows: $F_0 = 0$, $F_1 = 1$, and $F_{n+1} = F_n + F_{n-1}$ for $n \\geq 1$. There are $N$ positive integers $n$ strictly less than $10^{101}$ such that $n^2 + (n+1)^2$ is a multiple of 5 but $F_{n-1}^2 + F_n^2$ is not. How many prime factors does $N$ have, counted with multiplicity?\n",
      "------\n",
      "\n",
      "\n",
      "\n",
      "------\n",
      "1acac0\n",
      "Triangle $ABC$ has side length $AB = 120$ and circumradius $R = 100$. Let $D$ be the foot of the perpendicular from $C$ to the line $AB$. What is the greatest possible length of segment $CD$?\n",
      "If the final answer is a number larger than 1 million, take modulo 1000.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 16/16 [01:33<00:00,  5.85s/it, est. speed input: 21.91 toks/s, output: 164.46 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cecoce\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 3/3 [01:04<00:00, 21.54s/it, est. speed input: 55.55 toks/s, output: 30.71 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ce\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:14<00:00, 14.99s/it, est. speed input: 96.65 toks/s, output: 17.68 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ce\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:10<00:00, 10.62s/it, est. speed input: 165.79 toks/s, output: 15.64 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "co\n",
      "['190', '180', '100', '120', '{ {final_answer', '36', '36', '{ {max_CD', 'CD', '90', '180', '36', '60', '60', '152']\n",
      "36\n",
      "\n",
      "\n",
      "\n",
      "Triangle $ABC$ has side length $AB = 120$ and circumradius $R = 100$. Let $D$ be the foot of the perpendicular from $C$ to the line $AB$. What is the greatest possible length of segment $CD$?\n",
      "------\n",
      "\n",
      "\n",
      "\n",
      "------\n",
      "057f8a\n",
      "Three airline companies operate flights from Dodola island. Each company has a different schedule of departures. The first company departs every 100 days, the second every 120 days and the third every 150 days. What is the greatest positive integer $d$ for which it is true that there will be $d$ consecutive days without a flight from Dodola island, regardless of the departure times of the various airlines?\n",
      "------\n",
      "\n",
      "\n",
      "\n",
      "------\n",
      "1fce4b\n",
      "Find the three-digit number $n$ such that writing any other three-digit number $10^{2024}$ times in a row and $10^{2024}+2$ times in a row results in two numbers divisible by $n$.\n",
      "------\n",
      "\n",
      "\n",
      "\n",
      "------\n",
      "480182\n",
      "Let $ABC$ be a triangle with $BC=108$, $CA=126$, and $AB=39$. Point $X$ lies on segment $AC$ such that $BX$ bisects $\\angle CBA$. Let $\\omega$ be the circumcircle of triangle $ABX$. Let $Y$ be a point on $\\omega$ different from $X$ such that $CX=CY$. Line $XY$ meets $BC$ at $E$. The length of the segment $BE$ can be written as $\\frac{m}{n}$, where $m$ and $n$ are coprime positive integers. Find $m+n$.\n",
      "------\n",
      "\n",
      "\n",
      "\n",
      "------\n",
      "88c219\n",
      "For positive integers $x_1,\\ldots, x_n$ define $G(x_1, \\ldots, x_n)$ to be the sum of their $\\frac{n(n-1)}{2}$ pairwise greatest common divisors. We say that an integer $n \\geq 2$ is \\emph{artificial} if there exist $n$ different positive integers $a_1, ..., a_n$ such that \n",
      "\\[a_1 + \\cdots + a_n = G(a_1, \\ldots, a_n) +1.\\]\n",
      "Find the sum of all artificial integers $m$ in the range $2 \\leq m \\leq 40$.\n",
      "------\n",
      "\n",
      "\n",
      "\n",
      "------\n",
      "192e23\n",
      "Fred and George take part in a tennis tournament with $4046$ other players. In each round, the players are paired into $2024$ matches. How many ways are there to arrange the first round such that Fred and George do not have to play each other? (Two arrangements for the first round are \\textit{different} if there is a player with a different opponent in the two arrangements.)\n",
      "------\n",
      "\n",
      "\n",
      "\n",
      "------\n",
      "bbd91e\n",
      "Alice writes all positive integers from $1$ to $n$ on the board for some positive integer $n \\geq 11$. Bob then erases ten of them. The mean of the remaining numbers is $3000/37$. The sum of the numbers Bob erased is $S$. What is the remainder when $n \\times S$ is divided by $997$?\n",
      "------\n",
      "\n",
      "\n",
      "\n",
      "------\n",
      "349493\n",
      "We call a sequence $a_1, a_2, \\ldots$ of non-negative integers \\textit{delightful} if there exists a positive integer $N$ such that for all $n > N$, $a_n = 0$, and for all $i \\geq 1$, $a_i$ counts the number of multiples of $i$ in $a_1, a_2, \\ldots, a_N$. How many delightful sequences of non-negative integers are there?\n",
      "------\n",
      "\n",
      "\n",
      "\n",
      "------\n",
      "71beb6\n",
      "For a positive integer $n$, let $S(n)$ denote the sum of the digits of $n$ in base 10. Compute $S(S(1)+S(2)+\\cdots+S(N))$ with $N=10^{100}-2$.\n",
      "------\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inference_server = kaggle_evaluation.aimo_2_inference_server.AIMO2InferenceServer(predict)\n",
    "\n",
    "if os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n",
    "    inference_server.serve()\n",
    "else:\n",
    "    inference_server.run_local_gateway(\n",
    "        (\n",
    "#             '/kaggle/input/ai-mathematical-olympiad-progress-prize-2/test.csv',\n",
    "            'reference.csv',\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4249ba77",
   "metadata": {
    "papermill": {
     "duration": 0.012315,
     "end_time": "2024-10-29T09:47:53.897291",
     "exception": false,
     "start_time": "2024-10-29T09:47:53.884976",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaL4",
   "dataSources": [
    {
     "databundleVersionId": 9869096,
     "sourceId": 86023,
     "sourceType": "competition"
    },
    {
     "sourceId": 196750896,
     "sourceType": "kernelVersion"
    },
    {
     "modelId": 127417,
     "modelInstanceId": 118183,
     "sourceId": 139552,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30787,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 572.227101,
   "end_time": "2024-10-29T09:48:00.032322",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-10-29T09:38:27.805221",
   "version": "2.6.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "08e335aef70548d0a3333afdf068a9a7": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "1cfc1929eaa848d4a35255d0fc7c89d7": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "36f93f6a1b2d441dbdb38bf01b89ea32": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_9db4812505ff4859b243bf589d35226d",
        "IPY_MODEL_5efca5531d3a41e7abf6e9102c147fae",
        "IPY_MODEL_c79728f8d22e4aec9248f8cbb2c40c24"
       ],
       "layout": "IPY_MODEL_08e335aef70548d0a3333afdf068a9a7"
      }
     },
     "5efca5531d3a41e7abf6e9102c147fae": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_1cfc1929eaa848d4a35255d0fc7c89d7",
       "max": 11.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_d1a10db6e574470cbc5dfc805c9a7bae",
       "value": 11.0
      }
     },
     "61fc9c378b6e433ca6703297b65b4182": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "7ecbd04c41314d1990756749157fc112": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "9db4812505ff4859b243bf589d35226d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_7ecbd04c41314d1990756749157fc112",
       "placeholder": "​",
       "style": "IPY_MODEL_61fc9c378b6e433ca6703297b65b4182",
       "value": ""
      }
     },
     "b5fc5431e21c4d409b74c58b72ad95e4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "c79728f8d22e4aec9248f8cbb2c40c24": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_fc70bb48ece248b19e8961313f7250c4",
       "placeholder": "​",
       "style": "IPY_MODEL_b5fc5431e21c4d409b74c58b72ad95e4",
       "value": "Loading safetensors checkpoint shards: 100% Completed | 11/11 [04:41&lt;00:00, 27.61s/it]\n"
      }
     },
     "d1a10db6e574470cbc5dfc805c9a7bae": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "fc70bb48ece248b19e8961313f7250c4": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
