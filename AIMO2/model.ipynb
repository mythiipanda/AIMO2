{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import gc\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import random\n",
    "import scipy as sp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from glob import glob\n",
    "from pathlib import Path\n",
    "import joblib\n",
    "import pickle\n",
    "import itertools\n",
    "from tqdm.auto import tqdm\n",
    "import re\n",
    "import torch\n",
    "import vllm\n",
    "\n",
    "# Initialize LLM and Tokenizer\n",
    "llm = vllm.LLM(\n",
    "    \"Qwen/Qwen2.5-Math-7B-Instruct\",  # Ensure this model is supported\n",
    "    tensor_parallel_size=1,  # or 4 based on available resources\n",
    "    gpu_memory_utilization=0.95, \n",
    "    trust_remote_code=True,\n",
    "    dtype=\"half\", \n",
    "    enforce_eager=True,\n",
    ")\n",
    "tokenizer = llm.get_tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_vllm(requests, tokenizer, model):\n",
    "    sampling_params = vllm.SamplingParams(\n",
    "        temperature=0.00,\n",
    "        seed=42, \n",
    "        max_tokens=1024\n",
    "    )\n",
    "    responses = model.generate(requests, sampling_params=sampling_params, use_tqdm=False)\n",
    "    response_text_list = []\n",
    "    for response in responses:\n",
    "        # total_tokens += len(response.outputs[0].token_ids)\n",
    "        response_text_list.append(response.outputs[0].text)\n",
    "    return response_text_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tempfile\n",
    "import subprocess\n",
    "\n",
    "class PythonREPL:\n",
    "    def __init__(self, timeout=5):\n",
    "        self.timeout = timeout\n",
    "\n",
    "    def __call__(self, query):\n",
    "        with tempfile.TemporaryDirectory() as temp_dir:\n",
    "            temp_file_path = os.path.join(temp_dir, \"tmp.py\")\n",
    "            with open(temp_file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(query)\n",
    "            \n",
    "            try:\n",
    "                result = subprocess.run(\n",
    "                    [\"python3\", temp_file_path],\n",
    "                    capture_output=True,\n",
    "                    check=False,\n",
    "                    text=True,\n",
    "                    timeout=self.timeout,\n",
    "                )\n",
    "            except subprocess.TimeoutExpired:\n",
    "                return False, f\"Execution timed out after {self.timeout} seconds.\"\n",
    "\n",
    "            if result.returncode == 0:\n",
    "                output = result.stdout.strip()\n",
    "                return True, output\n",
    "            else:\n",
    "                error_msg = result.stderr.strip()\n",
    "                # Process the error message to remove the temporary file path\n",
    "                # This makes the error message cleaner and more user-friendly\n",
    "                error_lines = error_msg.split(\"\\n\")\n",
    "                cleaned_errors = []\n",
    "                for line in error_lines:\n",
    "                    if temp_file_path in line:\n",
    "                        # Remove the path from the error line\n",
    "                        line = line.replace(temp_file_path, \"<temporary_file>\")\n",
    "                    cleaned_errors.append(line)\n",
    "                cleaned_error_msg = \"\\n\".join(cleaned_errors)\n",
    "                return False, cleaned_error_msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "def extract_python_code(text):\n",
    "    pattern = r'```python\\s*(.*?)\\s*```'\n",
    "    matches = re.findall(pattern, text, re.DOTALL)\n",
    "    return \"\\n\\n\".join(matches)\n",
    "\n",
    "\n",
    "def process_python_code(query):\n",
    "    query = \"import math\\nimport numpy as np\\nimport sympy as sp\\n\" + query\n",
    "    current_rows = query.strip().split(\"\\n\")\n",
    "    new_rows = []\n",
    "    for row in current_rows:\n",
    "        new_rows.append(row)\n",
    "        if not row.startswith(\" \") and \"=\" in row:\n",
    "                variable_to_print = row.split(\"=\")[0].strip()\n",
    "                new_rows.append(f'print(f\"{{{variable_to_print}=}}\")')\n",
    "    return \"\\n\".join(new_rows)\n",
    "\n",
    "\n",
    "def extract_boxed_text(text):\n",
    "    pattern = r'oxed{(.*?)}'\n",
    "    matches = re.findall(pattern, text)\n",
    "    if not matches:\n",
    "        return \"\"\n",
    "    return matches[0]\n",
    "\n",
    "\n",
    "from collections import Counter\n",
    "import random\n",
    "def select_answer(answers):\n",
    "    counter = Counter()\n",
    "    for answer in answers:\n",
    "        try:\n",
    "            if int(answer) == float(answer):\n",
    "                counter[int(answer)] += 1 + random.random() / 1_000\n",
    "        except:\n",
    "            pass\n",
    "    if not counter:\n",
    "        return 210\n",
    "    _, answer = sorted([(v,k) for k,v in counter.items()], reverse=True)[0]\n",
    "    return answer%1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_params = SamplingParams(\n",
    "    temperature=1.0,              # randomness of the sampling\n",
    "    min_p=0.01,\n",
    "    skip_special_tokens=True,     # Whether to skip special tokens in the output.\n",
    "    max_tokens=1800,\n",
    "    stop=[\"```\\n\"],\n",
    "    include_stop_str_in_output=True,\n",
    ")\n",
    "\n",
    "def batch_message_generate(list_of_messages) -> list[list[dict]]:\n",
    "\n",
    "    list_of_texts = [\n",
    "        tokenizer.apply_chat_template(\n",
    "            conversation=messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True\n",
    "        )\n",
    "        for messages in list_of_messages\n",
    "    ]\n",
    "    \n",
    "    request_output = llm.generate(\n",
    "        prompts=list_of_texts,\n",
    "        sampling_params=sampling_params,\n",
    "    )\n",
    "    \n",
    "    for messages, single_request_output in zip(list_of_messages, request_output):\n",
    "        # print()\n",
    "        # print(single_request_output.outputs[0].text)\n",
    "        # print()\n",
    "        messages.append({'role': 'assistant', 'content': single_request_output.outputs[0].text})\n",
    "\n",
    "    return list_of_messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_message_filter(list_of_messages) -> tuple[list[list[dict]], list[str]]:\n",
    "    extracted_answers = []\n",
    "    list_of_messages_to_keep = []\n",
    "    for messages in list_of_messages:\n",
    "        answer = extract_boxed_text(messages[-1]['content'])\n",
    "        if answer:\n",
    "            extracted_answers.append(answer)\n",
    "        else:\n",
    "            list_of_messages_to_keep.append(messages)\n",
    "    return list_of_messages_to_keep, extracted_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_message_execute(list_of_messages) -> list[list[dict]]:\n",
    "    for messages in list_of_messages:\n",
    "        python_code = extract_python_code(messages[-1]['content'])\n",
    "        python_code = process_python_code(python_code)\n",
    "        # print('\\n\\n' + python_code + '\\n\\n')\n",
    "        try:\n",
    "            print('c', end='')\n",
    "            is_successful, output = PythonREPL()(python_code)\n",
    "            if is_successful:\n",
    "                print('o', end='')\n",
    "            else:\n",
    "                print('e', end='')\n",
    "        except Exception as e:\n",
    "            print('f', end='')\n",
    "            output = str(e)\n",
    "        # print(output)\n",
    "        messages.append({'role': 'user', 'content': output})\n",
    "    print()\n",
    "    return list_of_messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_starter_messages(question, index):\n",
    "    cycle_size = 4\n",
    "    if False:\n",
    "        pass\n",
    "    elif index % cycle_size == 3:\n",
    "        return [\n",
    "            {\"role\": \"user\", \"content\": \"Translate this problem into Python code.\\n\\n\" + question + \"\\n\\nStart by importing numpy and sympy. If you get a confident answer after running the sympy code, put your final answer within \\boxed{}\"}\n",
    "        ]\n",
    "    elif index % cycle_size == 2:\n",
    "        return [\n",
    "            {\"role\": \"user\", \"content\": \"Translate the following problem into sympy.\\n\\n\" + question + \"\\n\\nStart by importing sympy. If you get a confident answer after running the sympy code, put your final answer within \\boxed{}\"}\n",
    "        ]\n",
    "    elif index % cycle_size == 1:\n",
    "        # https://github.com/QwenLM/Qwen2.5-Math?tab=readme-ov-file#-hugging-face-transformers\n",
    "        return [\n",
    "            {\"role\": \"system\", \"content\": \"Please reason step by step, and put your final answer within \\\\boxed{}.\"},\n",
    "            {\"role\": \"user\", \"content\": question}\n",
    "        ]\n",
    "    else:\n",
    "        # https://github.com/QwenLM/Qwen2.5-Math?tab=readme-ov-file#-hugging-face-transformers\n",
    "        return [\n",
    "            {\"role\": \"system\", \"content\": \"Please integrate natural language reasoning with Python programs to solve the problem above, and put your final answer within \\\\boxed{}.\"},\n",
    "            {\"role\": \"user\", \"content\": question}\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_for_question(question: str) -> int:\n",
    "    import os\n",
    "    if not os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n",
    "        if question != \"Triangle $ABC$ has side length $AB = 120$ and circumradius $R = 100$. Let $D$ be the foot of the perpendicular from $C$ to the line $AB$. What is the greatest possible length of segment $CD$?\":\n",
    "            return 210\n",
    "    \n",
    "    question += \"\\nIf the final answer is a number larger than 1 million, take modulo 1000.\"\n",
    "    print(question)\n",
    "\n",
    "    list_of_messages = [create_starter_messages(question, index) for index in range(16)]\n",
    "\n",
    "    all_extracted_answers = []\n",
    "    for _ in range(4):\n",
    "        list_of_messages = batch_message_generate(list_of_messages)\n",
    "        list_of_messages, extracted_answers = batch_message_filter(list_of_messages)\n",
    "        all_extracted_answers.extend(extracted_answers)\n",
    "        if not list_of_messages:\n",
    "            break\n",
    "        list_of_messages = batch_message_execute(list_of_messages)\n",
    "\n",
    "    print(all_extracted_answers)\n",
    "    answer = select_answer(all_extracted_answers)\n",
    "    print(answer)\n",
    "\n",
    "    print(\"\\n\\n\")\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(id_: pl.DataFrame, question: pl.DataFrame) -> pl.DataFrame | pd.DataFrame:\n",
    "    id_ = id_.item(0)\n",
    "    print(\"------\")\n",
    "    print(id_)\n",
    "    \n",
    "    question = question.item(0)\n",
    "    answer = predict_for_question(question)\n",
    "    print(question)\n",
    "    print(\"------\\n\\n\\n\")\n",
    "    return pl.DataFrame({'id': id_, 'answer': answer})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
