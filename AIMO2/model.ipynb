{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install polars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "# torch.cuda.memory_summary(device=None, abbreviated=False)\n",
    "# torch.cuda.is_available()\n",
    "# torch.cuda.device_count()\n",
    "# torch.cuda.current_device()\n",
    "# torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory cleared on all GPUs.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import gc\n",
    "\n",
    "# Delete all references to models\n",
    "for obj in list(globals().values()):\n",
    "    if torch.is_tensor(obj) or isinstance(obj, torch.nn.Module):\n",
    "        del obj\n",
    "\n",
    "# Collect garbage\n",
    "gc.collect()\n",
    "\n",
    "# Free up all GPU memory\n",
    "for i in range(torch.cuda.device_count()):\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.reset_peak_memory_stats(i)\n",
    "\n",
    "print(\"Memory cleared on all GPUs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1,2,3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !kill -9 1633693 1633694 1633695"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Oct 29 17:28:21 2024       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.54.14              Driver Version: 550.54.14      CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 2080 Ti     Off |   00000000:67:00.0 Off |                  N/A |\n",
      "| 34%   40C    P8             27W /  250W |       0MiB /  11264MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   1  NVIDIA GeForce RTX 2080 Ti     Off |   00000000:68:00.0 Off |                  N/A |\n",
      "| 35%   40C    P8             24W /  250W |       0MiB /  11264MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   2  NVIDIA GeForce RTX 2080 Ti     Off |   00000000:B5:00.0 Off |                  N/A |\n",
      "| 34%   40C    P8             26W /  250W |       0MiB /  11264MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   3  NVIDIA GeForce RTX 2080 Ti     Off |   00000000:B6:00.0 Off |                  N/A |\n",
      "| 37%   41C    P8             21W /  250W |       0MiB /  11264MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true,
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 10-29 17:28:33 config.py:1668] Casting torch.bfloat16 to torch.float16.\n",
      "INFO 10-29 17:28:40 config.py:905] Defaulting to use mp for distributed inference\n",
      "WARNING 10-29 17:28:40 config.py:395] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
      "INFO 10-29 17:28:40 llm_engine.py:237] Initializing an LLM engine (v0.6.3.post1) with config: model='Qwen/Qwen2.5-Math-7B-Instruct', speculative_config=None, tokenizer='Qwen/Qwen2.5-Math-7B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=Qwen/Qwen2.5-Math-7B-Instruct, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=False, use_cached_outputs=False, mm_processor_kwargs=None)\n",
      "WARNING 10-29 17:28:40 multiproc_gpu_executor.py:53] Reducing Torch parallelism from 10 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 10-29 17:28:40 custom_cache_manager.py:17] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager\n",
      "INFO 10-29 17:28:41 selector.py:224] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
      "INFO 10-29 17:28:41 selector.py:115] Using XFormers backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1658975)\u001b[0;0m INFO 10-29 17:28:41 selector.py:224] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1658975)\u001b[0;0m INFO 10-29 17:28:41 selector.py:115] Using XFormers backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1658974)\u001b[0;0m INFO 10-29 17:28:41 selector.py:224] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1658974)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=1658973)\u001b[0;0m INFO 10-29 17:28:41 selector.py:115] Using XFormers backend.\n",
      "INFO 10-29 17:28:41 selector.py:224] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1658973)\u001b[0;0m INFO 10-29 17:28:41 selector.py:115] Using XFormers backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1658973)\u001b[0;0m INFO 10-29 17:28:41 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1658974)\u001b[0;0m INFO 10-29 17:28:41 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1658975)\u001b[0;0m INFO 10-29 17:28:41 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n",
      "INFO 10-29 17:28:42 utils.py:1008] Found nccl from library libnccl.so.2\n",
      "INFO 10-29 17:28:42 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1658975)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=1658973)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=1658974)\u001b[0;0m INFO 10-29 17:28:42 utils.py:1008] Found nccl from library libnccl.so.2\n",
      "INFO 10-29 17:28:42 utils.py:1008] Found nccl from library libnccl.so.2\n",
      "INFO 10-29 17:28:42 utils.py:1008] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1658975)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=1658974)\u001b[0;0m INFO 10-29 17:28:42 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1658973)\u001b[0;0m INFO 10-29 17:28:42 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "INFO 10-29 17:28:42 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "WARNING 10-29 17:28:42 custom_all_reduce.py:132] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1658973)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=1658974)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=1658975)\u001b[0;0m WARNING 10-29 17:28:42 custom_all_reduce.py:132] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 10-29 17:28:42 custom_all_reduce.py:132] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 10-29 17:28:42 custom_all_reduce.py:132] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "INFO 10-29 17:28:42 shm_broadcast.py:241] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1, 2, 3], buffer=<vllm.distributed.device_communicators.shm_broadcast.ShmRingBuffer object at 0x7f14d26acc90>, local_subscribe_port=59937, remote_subscribe_port=None)\n",
      "INFO 10-29 17:28:42 model_runner.py:1056] Starting to load model Qwen/Qwen2.5-Math-7B-Instruct...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1658974)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=1658975)\u001b[0;0m INFO 10-29 17:28:42 model_runner.py:1056] Starting to load model Qwen/Qwen2.5-Math-7B-Instruct...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1658973)\u001b[0;0m INFO 10-29 17:28:42 model_runner.py:1056] Starting to load model Qwen/Qwen2.5-Math-7B-Instruct...\n",
      "INFO 10-29 17:28:42 model_runner.py:1056] Starting to load model Qwen/Qwen2.5-Math-7B-Instruct...\n",
      "INFO 10-29 17:28:42 selector.py:224] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
      "INFO 10-29 17:28:42 selector.py:115] Using XFormers backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1658974)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=1658973)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=1658975)\u001b[0;0m INFO 10-29 17:28:42 selector.py:224] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
      "INFO 10-29 17:28:42 selector.py:224] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
      "INFO 10-29 17:28:42 selector.py:224] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1658975)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=1658974)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=1658973)\u001b[0;0m INFO 10-29 17:28:42 selector.py:115] Using XFormers backend.\n",
      "INFO 10-29 17:28:42 selector.py:115] Using XFormers backend.\n",
      "INFO 10-29 17:28:42 selector.py:115] Using XFormers backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1658974)\u001b[0;0m INFO 10-29 17:28:42 weight_utils.py:243] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1658975)\u001b[0;0m INFO 10-29 17:28:42 weight_utils.py:243] Using model weights format ['*.safetensors']\n",
      "INFO 10-29 17:28:42 weight_utils.py:243] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1658973)\u001b[0;0m INFO 10-29 17:28:42 weight_utils.py:243] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8e2e2665ba2412c861567fd5f62a305",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorkerProcess pid=1658974)\u001b[0;0m INFO 10-29 17:28:47 model_runner.py:1067] Loading model weights took 3.5478 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1658973)\u001b[0;0m INFO 10-29 17:28:47 model_runner.py:1067] Loading model weights took 3.5478 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1658975)\u001b[0;0m INFO 10-29 17:28:47 model_runner.py:1067] Loading model weights took 3.5478 GB\n",
      "INFO 10-29 17:28:47 model_runner.py:1067] Loading model weights took 3.5478 GB\n",
      "INFO 10-29 17:28:49 distributed_gpu_executor.py:57] # GPU blocks: 21742, # CPU blocks: 18724\n",
      "INFO 10-29 17:28:49 distributed_gpu_executor.py:61] Maximum concurrency for 4096 tokens per request: 84.93x\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import gc\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import random\n",
    "import scipy as sp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from glob import glob\n",
    "from pathlib import Path\n",
    "import joblib\n",
    "import pickle\n",
    "import itertools\n",
    "from tqdm.auto import tqdm\n",
    "import re\n",
    "import torch\n",
    "import vllm\n",
    "import polars as pl\n",
    "\n",
    "# Initialize LLM and Tokenizer\n",
    "llm = vllm.LLM(\n",
    "    \"Qwen/Qwen2.5-Math-7B-Instruct\",  # Ensure this model is supported\n",
    "    tensor_parallel_size=4,  # or 4 based on available resources\n",
    "    gpu_memory_utilization=0.95, \n",
    "    trust_remote_code=True,\n",
    "    dtype=\"half\", \n",
    "    enforce_eager=True,\n",
    ")\n",
    "tokenizer = llm.get_tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_vllm(requests, tokenizer, model):\n",
    "    sampling_params = vllm.SamplingParams(\n",
    "        temperature=0.00,\n",
    "        seed=42, \n",
    "        max_tokens=1024\n",
    "    )\n",
    "    responses = model.generate(requests, sampling_params=sampling_params, use_tqdm=False)\n",
    "    response_text_list = []\n",
    "    for response in responses:\n",
    "        # total_tokens += len(response.outputs[0].token_ids)\n",
    "        response_text_list.append(response.outputs[0].text)\n",
    "    return response_text_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tempfile\n",
    "import subprocess\n",
    "\n",
    "class PythonREPL:\n",
    "    def __init__(self, timeout=5):\n",
    "        self.timeout = timeout\n",
    "\n",
    "    def __call__(self, query):\n",
    "        with tempfile.TemporaryDirectory() as temp_dir:\n",
    "            temp_file_path = os.path.join(temp_dir, \"tmp.py\")\n",
    "            with open(temp_file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(query)\n",
    "            \n",
    "            try:\n",
    "                result = subprocess.run(\n",
    "                    [\"python3\", temp_file_path],\n",
    "                    capture_output=True,\n",
    "                    check=False,\n",
    "                    text=True,\n",
    "                    timeout=self.timeout,\n",
    "                )\n",
    "            except subprocess.TimeoutExpired:\n",
    "                return False, f\"Execution timed out after {self.timeout} seconds.\"\n",
    "\n",
    "            if result.returncode == 0:\n",
    "                output = result.stdout.strip()\n",
    "                return True, output\n",
    "            else:\n",
    "                error_msg = result.stderr.strip()\n",
    "                # Process the error message to remove the temporary file path\n",
    "                # This makes the error message cleaner and more user-friendly\n",
    "                error_lines = error_msg.split(\"\\n\")\n",
    "                cleaned_errors = []\n",
    "                for line in error_lines:\n",
    "                    if temp_file_path in line:\n",
    "                        # Remove the path from the error line\n",
    "                        line = line.replace(temp_file_path, \"<temporary_file>\")\n",
    "                    cleaned_errors.append(line)\n",
    "                cleaned_error_msg = \"\\n\".join(cleaned_errors)\n",
    "                return False, cleaned_error_msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "def extract_python_code(text):\n",
    "    pattern = r'```python\\s*(.*?)\\s*```'\n",
    "    matches = re.findall(pattern, text, re.DOTALL)\n",
    "    return \"\\n\\n\".join(matches)\n",
    "\n",
    "\n",
    "def process_python_code(query):\n",
    "    query = \"import math\\nimport numpy as np\\nimport sympy as sp\\n\" + query\n",
    "    current_rows = query.strip().split(\"\\n\")\n",
    "    new_rows = []\n",
    "    for row in current_rows:\n",
    "        new_rows.append(row)\n",
    "        if not row.startswith(\" \") and \"=\" in row:\n",
    "                variable_to_print = row.split(\"=\")[0].strip()\n",
    "                new_rows.append(f'print(f\"{{{variable_to_print}=}}\")')\n",
    "    return \"\\n\".join(new_rows)\n",
    "\n",
    "\n",
    "def extract_boxed_text(text):\n",
    "    pattern = r'oxed{(.*?)}'\n",
    "    matches = re.findall(pattern, text)\n",
    "    if not matches:\n",
    "        return \"\"\n",
    "    return matches[0]\n",
    "\n",
    "\n",
    "from collections import Counter\n",
    "import random\n",
    "def select_answer(answers):\n",
    "    counter = Counter()\n",
    "    for answer in answers:\n",
    "        try:\n",
    "            if int(answer) == float(answer):\n",
    "                counter[int(answer)] += 1 + random.random() / 1_000\n",
    "        except:\n",
    "            pass\n",
    "    if not counter:\n",
    "        return 210\n",
    "    _, answer = sorted([(v,k) for k,v in counter.items()], reverse=True)[0]\n",
    "    return answer%1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_params = vllm.SamplingParams(\n",
    "    temperature=1.0,              # randomness of the sampling\n",
    "    min_p=0.01,\n",
    "    skip_special_tokens=True,     # Whether to skip special tokens in the output.\n",
    "    max_tokens=1800,\n",
    "    stop=[\"```\\n\"],\n",
    "    include_stop_str_in_output=True,\n",
    ")\n",
    "\n",
    "def batch_message_generate(list_of_messages) -> list[list[dict]]:\n",
    "\n",
    "    list_of_texts = [\n",
    "        tokenizer.apply_chat_template(\n",
    "            conversation=messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True\n",
    "        )\n",
    "        for messages in list_of_messages\n",
    "    ]\n",
    "    \n",
    "    request_output = llm.generate(\n",
    "        prompts=list_of_texts,\n",
    "        sampling_params=sampling_params,\n",
    "    )\n",
    "    \n",
    "    for messages, single_request_output in zip(list_of_messages, request_output):\n",
    "        # print()\n",
    "        # print(single_request_output.outputs[0].text)\n",
    "        # print()\n",
    "        messages.append({'role': 'assistant', 'content': single_request_output.outputs[0].text})\n",
    "\n",
    "    return list_of_messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_message_filter(list_of_messages) -> tuple[list[list[dict]], list[str]]:\n",
    "    extracted_answers = []\n",
    "    list_of_messages_to_keep = []\n",
    "    for messages in list_of_messages:\n",
    "        answer = extract_boxed_text(messages[-1]['content'])\n",
    "        if answer:\n",
    "            extracted_answers.append(answer)\n",
    "        else:\n",
    "            list_of_messages_to_keep.append(messages)\n",
    "    return list_of_messages_to_keep, extracted_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_message_execute(list_of_messages) -> list[list[dict]]:\n",
    "    for messages in list_of_messages:\n",
    "        python_code = extract_python_code(messages[-1]['content'])\n",
    "        python_code = process_python_code(python_code)\n",
    "        # print('\\n\\n' + python_code + '\\n\\n')\n",
    "        try:\n",
    "            print('c', end='')\n",
    "            is_successful, output = PythonREPL()(python_code)\n",
    "            if is_successful:\n",
    "                print('o', end='')\n",
    "            else:\n",
    "                print('e', end='')\n",
    "        except Exception as e:\n",
    "            print('f', end='')\n",
    "            output = str(e)\n",
    "        # print(output)\n",
    "        messages.append({'role': 'user', 'content': output})\n",
    "    print()\n",
    "    return list_of_messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_starter_messages(question, index):\n",
    "    cycle_size = 4\n",
    "    if False:\n",
    "        pass\n",
    "    elif index % cycle_size == 3:\n",
    "        return [\n",
    "            {\"role\": \"user\", \"content\": \"Translate this problem into Python code.\\n\\n\" + question + \"\\n\\nStart by importing numpy and sympy. If you get a confident answer after running the sympy code, put your final answer within \\boxed{}\"}\n",
    "        ]\n",
    "    elif index % cycle_size == 2:\n",
    "        return [\n",
    "            {\"role\": \"user\", \"content\": \"Translate the following problem into sympy.\\n\\n\" + question + \"\\n\\nStart by importing sympy. If you get a confident answer after running the sympy code, put your final answer within \\boxed{}\"}\n",
    "        ]\n",
    "    elif index % cycle_size == 1:\n",
    "        # https://github.com/QwenLM/Qwen2.5-Math?tab=readme-ov-file#-hugging-face-transformers\n",
    "        return [\n",
    "            {\"role\": \"system\", \"content\": \"Please reason step by step, and put your final answer within \\\\boxed{}.\"},\n",
    "            {\"role\": \"user\", \"content\": question}\n",
    "        ]\n",
    "    else:\n",
    "        # https://github.com/QwenLM/Qwen2.5-Math?tab=readme-ov-file#-hugging-face-transformers\n",
    "        return [\n",
    "            {\"role\": \"system\", \"content\": \"Please integrate natural language reasoning with Python programs to solve the problem above, and put your final answer within \\\\boxed{}.\"},\n",
    "            {\"role\": \"user\", \"content\": question}\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_for_question(question: str) -> int:\n",
    "    import os\n",
    "    # if not os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n",
    "    #     if question != \"Triangle $ABC$ has side length $AB = 120$ and circumradius $R = 100$. Let $D$ be the foot of the perpendicular from $C$ to the line $AB$. What is the greatest possible length of segment $CD$?\":\n",
    "    #         return 210\n",
    "    \n",
    "    # question += \"\\nIf the final answer is a number larger than 1 million, take modulo 1000.\"\n",
    "    print(question)\n",
    "\n",
    "    list_of_messages = [create_starter_messages(question, index) for index in range(16)]\n",
    "\n",
    "    all_extracted_answers = []\n",
    "    for _ in range(4):\n",
    "        list_of_messages = batch_message_generate(list_of_messages)\n",
    "        list_of_messages, extracted_answers = batch_message_filter(list_of_messages)\n",
    "        all_extracted_answers.extend(extracted_answers)\n",
    "        if not list_of_messages:\n",
    "            break\n",
    "        list_of_messages = batch_message_execute(list_of_messages)\n",
    "\n",
    "    print(all_extracted_answers)\n",
    "    answer = select_answer(all_extracted_answers)\n",
    "    print(answer)\n",
    "\n",
    "    print(\"\\n\\n\")\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace this function with your inference code.\n",
    "# The function should return a single integer between 0 and 999, inclusive.\n",
    "# Each prediction (except the very first) must be returned within 30 minutes of the question being provided.\n",
    "def predict(id_: pl.DataFrame, question: pl.DataFrame) -> pl.DataFrame | pd.DataFrame:\n",
    "    ids = id_['id'].to_list()  # Extract the 'id' column as a list\n",
    "    questions = question['problem'].to_list()  # Extract the 'problem' column as a list\n",
    "    \n",
    "    answers = []\n",
    "    for q in questions:\n",
    "        answer = predict_for_question(q)\n",
    "        answers.append(answer)\n",
    "    \n",
    "    result_df = pl.DataFrame({'id': ids, 'answer': answers})\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Three airline companies operate flights from Dodola island. Each company has a different schedule of departures. The first company departs every 100 days, the second every 120 days and the third every 150 days. What is the greatest positive integer $d$ for which it is true that there will be $d$ consecutive days without a flight from Dodola island, regardless of the departure times of the various airlines?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|█| 16/16 [00:43<00:00,  2.72s/it, est. speed input: 51.3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cecococecoceco\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|█| 7/7 [00:43<00:00,  6.19s/it, est. speed input: 152.25\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cococece\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|█| 4/4 [00:43<00:00, 10.91s/it, est. speed input: 201.22\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cocococo\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   0%| | 0/4 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, o"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 10-29 17:34:08 scheduler.py:895] Input prompt (4157 tokens) is too long and exceeds limit of 4096\n",
      "WARNING 10-29 17:34:08 scheduler.py:895] Input prompt (4110 tokens) is too long and exceeds limit of 4096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|█| 4/4 [00:40<00:00, 10.15s/it, est. speed input: 302.28\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cococo\n",
      "['599', '59', '599', '599', '599', '599', '599', '599', '599', '9', '23', '259', '23']\n",
      "599\n",
      "\n",
      "\n",
      "\n",
      "Fred and George take part in a tennis tournament with $4046$ other players. In each round, the players are paired into $2024$ matches. How many ways are there to arrange the first round such that Fred and George do not have to play each other? (Two arrangements for the first round are \\textit{different} if there is a player with a different opponent in the two arrangements.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|█| 16/16 [00:44<00:00,  2.79s/it, est. speed input: 48.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cecococecococecececoco\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|█| 11/11 [00:47<00:00,  4.35s/it, est. speed input: 360.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cocococecococececoco\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   0%| | 0/10 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 10-29 17:36:40 scheduler.py:895] Input prompt (26954 tokens) is too long and exceeds limit of 4096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|█| 10/10 [00:17<00:00,  1.77s/it, est. speed input: 3054\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cocococecococecoco\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   0%| | 0/9 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, o"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 10-29 17:37:05 scheduler.py:895] Input prompt (26964 tokens) is too long and exceeds limit of 4096\n",
      "WARNING 10-29 17:37:05 scheduler.py:895] Input prompt (4115 tokens) is too long and exceeds limit of 4096\n",
      "WARNING 10-29 17:37:05 scheduler.py:895] Input prompt (4117 tokens) is too long and exceeds limit of 4096\n",
      "WARNING 10-29 17:37:05 scheduler.py:895] Input prompt (4114 tokens) is too long and exceeds limit of 4096\n",
      "WARNING 10-29 17:37:05 scheduler.py:895] Input prompt (4111 tokens) is too long and exceeds limit of 4096\n",
      "WARNING 10-29 17:37:05 scheduler.py:895] Input prompt (4109 tokens) is too long and exceeds limit of 4096\n",
      "WARNING 10-29 17:37:05 scheduler.py:895] Input prompt (4110 tokens) is too long and exceeds limit of 4096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|█| 9/9 [00:07<00:00,  1.28it/s, est. speed input: 7743.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cocococecocococo\n",
      "['2023', '4046', '30335323850306856154144779494719699254967268758092096', '\\\\frac{2023! \\\\cdot 4046!', '2024', '\\\\frac{4046 \\\\cdot 4046!', '2036436', '\\\\frac{2027!']\n",
      "436\n",
      "\n",
      "\n",
      "\n",
      "Triangle $ABC$ has side length $AB = 120$ and circumradius $R = 100$. Let $D$ be the foot of the perpendicular from $C$ to the line $AB$. What is the greatest possible length of segment $CD$?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|█| 16/16 [00:45<00:00,  2.82s/it, est. speed input: 37.8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cocococococo\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|█| 6/6 [00:45<00:00,  7.58s/it, est. speed input: 232.54\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cocococo\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|█| 4/4 [00:09<00:00,  2.40s/it, est. speed input: 1615.2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cocococo\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   0%| | 0/4 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, o"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 10-29 17:39:09 scheduler.py:895] Input prompt (4117 tokens) is too long and exceeds limit of 4096\n",
      "WARNING 10-29 17:39:09 scheduler.py:895] Input prompt (4119 tokens) is too long and exceeds limit of 4096\n",
      "WARNING 10-29 17:39:09 scheduler.py:895] Input prompt (4118 tokens) is too long and exceeds limit of 4096\n",
      "WARNING 10-29 17:39:09 scheduler.py:895] Input prompt (4113 tokens) is too long and exceeds limit of 4096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|█| 4/4 [00:00<00:00, 1298.95it/s, est. speed input: 5465"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ocococo\n",
      "['60\\\\sqrt{3', '36', '60', '15', '60', '160', '\\\\frac{600', '360', '18', '16', '180', '60']\n",
      "60\n",
      "\n",
      "\n",
      "\n",
      "Find the three-digit number $n$ such that writing any other three-digit number $10^{2024}$ times in a row and $10^{2024}+2$ times in a row results in two numbers divisible by $n$.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|█| 16/16 [00:44<00:00,  2.80s/it, est. speed input: 37.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cocococecococeco\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|█| 8/8 [00:46<00:00,  5.82s/it, est. speed input: 269.84\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cococococo\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|█| 5/5 [00:10<00:00,  2.16s/it, est. speed input: 1786.4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cococococo\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   0%| | 0/5 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, o"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 10-29 17:41:08 scheduler.py:895] Input prompt (4112 tokens) is too long and exceeds limit of 4096\n",
      "WARNING 10-29 17:41:08 scheduler.py:895] Input prompt (4112 tokens) is too long and exceeds limit of 4096\n",
      "WARNING 10-29 17:41:08 scheduler.py:895] Input prompt (4114 tokens) is too long and exceeds limit of 4096\n",
      "WARNING 10-29 17:41:08 scheduler.py:895] Input prompt (4121 tokens) is too long and exceeds limit of 4096\n",
      "WARNING 10-29 17:41:08 scheduler.py:895] Input prompt (4121 tokens) is too long and exceeds limit of 4096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|█| 5/5 [00:00<00:00, 921.46it/s, est. speed input: 39125"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ococococo\n",
      "['143', '111', '37', '37', '111', '345', '1009', '37', '633', '1489', '135']\n",
      "37\n",
      "\n",
      "\n",
      "\n",
      "We call a sequence $a_1, a_2, \\ldots$ of non-negative integers \\textit{delightful} if there exists a positive integer $N$ such that for all $n > N$, $a_n = 0$, and for all $i \\geq 1$, $a_i$ counts the number of multiples of $i$ in $a_1, a_2, \\ldots, a_N$. How many delightful sequences of non-negative integers are there?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|█| 16/16 [00:44<00:00,  2.81s/it, est. speed input: 55.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cococeco\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|█| 4/4 [00:43<00:00, 10.94s/it, est. speed input: 136.55\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coceco\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|█| 3/3 [00:06<00:00,  2.01s/it, est. speed input: 1565.2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coco\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   0%| | 0/2 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, o"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 10-29 17:42:52 scheduler.py:895] Input prompt (4112 tokens) is too long and exceeds limit of 4096\n",
      "WARNING 10-29 17:42:52 scheduler.py:895] Input prompt (4116 tokens) is too long and exceeds limit of 4096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|█| 2/2 [00:00<00:00, 841.98it/s, est. speed input: 37240"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "oco\n",
      "['\\\\infty', '1', '2', '2', '\\\\infty', '2', '2', '2', '1', '2', '3', '3', '2', '3']\n",
      "2\n",
      "\n",
      "\n",
      "\n",
      "Let $ABC$ be a triangle with $BC=108$, $CA=126$, and $AB=39$. Point $X$ lies on segment $AC$ such that $BX$ bisects $\\angle CBA$. Let $\\omega$ be the circumcircle of triangle $ABX$. Let $Y$ be a point on $\\omega$ different from $X$ such that $CX=CY$. Line $XY$ meets $BC$ at $E$. The length of the segment $BE$ can be written as $\\frac{m}{n}$, where $m$ and $n$ are coprime positive integers. Find $m+n$.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  94%|▉| 15/16 [00:31<00:02,  2.37s/it, est. speed input: 90.9"
     ]
    }
   ],
   "source": [
    "reference_df = pl.read_csv('reference.csv')\n",
    "reference_df = reference_df.drop('answer')\n",
    "id_df = reference_df.select('id')\n",
    "question_df = reference_df.select('problem')\n",
    "result_df = predict(id_df, question_df)\n",
    "result_df.to_csv(\"submission2.csv\")\n",
    "# Print the result\n",
    "print(result_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
